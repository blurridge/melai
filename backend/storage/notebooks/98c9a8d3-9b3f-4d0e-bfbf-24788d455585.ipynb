{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d6da9d3",
   "metadata": {},
   "source": [
    "# Machine Learning Model: logistic_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb80d744",
   "metadata": {},
   "source": [
    "\n",
    "        This notebook was automatically generated to train a logistic_regression model for classification.\n",
    "        \n",
    "        * Dataset: ba6aa228-bd5e-457f-9b99-bee9d2a908a5\n",
    "        * Target column: Survived\n",
    "        * Generated on: 2025-03-11 09:14:33\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e9391",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # Import necessary libraries\n",
    "        import pandas as pd\n",
    "        import numpy as np\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "        from sklearn.compose import ColumnTransformer\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sklearn.impute import SimpleImputer\n",
    "        import shap\n",
    "        \n",
    "        # Set plot style\n",
    "        plt.style.use('seaborn-whitegrid')\n",
    "        sns.set(font_scale=1.2)\n",
    "        \n",
    "        # Suppress warnings\n",
    "        import warnings\n",
    "        warnings.filterwarnings('ignore')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64820107",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # Load the dataset\n",
    "        df = pd.read_csv('../datasets/ba6aa228-bd5e-457f-9b99-bee9d2a908a5.csv', index_col=0) if os.path.exists('../datasets/ba6aa228-bd5e-457f-9b99-bee9d2a908a5.csv') else              pd.read_excel('../datasets/ba6aa228-bd5e-457f-9b99-bee9d2a908a5.xlsx', index_col=0) if os.path.exists('../datasets/ba6aa228-bd5e-457f-9b99-bee9d2a908a5.xlsx') else              pd.read_json('../datasets/ba6aa228-bd5e-457f-9b99-bee9d2a908a5.json', orient='records')\n",
    "             \n",
    "        # Display basic information\n",
    "        print(\"Dataset shape:\", df.shape)\n",
    "        df.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d83591c",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfbdfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # Display basic statistics\n",
    "        df.describe()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683be769",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # Check for missing values\n",
    "        missing_values = df.isnull().sum()\n",
    "        missing_values = missing_values[missing_values > 0]\n",
    "        if len(missing_values) > 0:\n",
    "            print(\"Missing values by column:\")\n",
    "            print(missing_values)\n",
    "        else:\n",
    "            print(\"No missing values found.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9acf90a",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54422176",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            # Target class distribution\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.countplot(y=df['Survived'])\n",
    "            plt.title('Distribution of Target Classes')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Feature distribution by class\n",
    "            features = [f.name for f in request.features if f.use and pd.api.types.is_numeric_dtype(df[f.name])]\n",
    "            if len(features) > 0:\n",
    "                fig, axes = plt.subplots(len(features), 1, figsize=(12, 4*len(features)))\n",
    "                for i, col_name in enumerate(features[:3]):\n",
    "                    if len(features) > 1:\n",
    "                        ax = axes[i]\n",
    "                    else:\n",
    "                        ax = axes\n",
    "                    sns.boxplot(x='Survived', y=col_name, data=df, ax=ax)\n",
    "                    ax.set_title(f'{col_name} by Survived')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af7fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Import necessary libraries\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    from sklearn.metrics import *\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a2c7fe",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e76f012",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Split data into features and target\n",
    "    X = df.drop(columns=['Survived'])\n",
    "    y = df['Survived']\n",
    "    \n",
    "    # Identify numerical and categorical features\n",
    "    numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"Numerical features: {numerical_features}\")\n",
    "    print(f\"Categorical features: {categorical_features}\")\n",
    "    \n",
    "    # Check for complex object features that may cause issues\n",
    "    for col in categorical_features:\n",
    "        sample = df[col].iloc[0]\n",
    "        if isinstance(sample, dict) or isinstance(sample, list):\n",
    "            print(f\"Warning: Column {col} contains complex objects which may not work with standard transformations.\")\n",
    "            print(f\"Sample value: {sample}\")\n",
    "            print(f\"Consider extracting specific fields from this object or excluding it from your model.\")\n",
    "    \n",
    "# Apply one-hot encoding to Name\n",
    "if 'Name' in categorical_features:\n",
    "    # Check if the feature contains complex values that need special handling\n",
    "    if X['Name'].apply(lambda x: isinstance(x, (dict, list))).any():\n",
    "        print(f\"Warning: Cannot apply one-hot encoding to complex objects in Name\")\n",
    "        print(f\"Converting to string representation first\")\n",
    "        X['Name'] = X['Name'].astype(str)\n",
    "    try:\n",
    "        onehot = pd.get_dummies(X['Name'], prefix='Name')\n",
    "        X = pd.concat([X.drop('Name', axis=1), onehot], axis=1)\n",
    "        print(f\"Created one-hot encoding for: Name\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error one-hot encoding Name: {e}\")\n",
    "# Apply one-hot encoding to Sex\n",
    "if 'Sex' in categorical_features:\n",
    "    # Check if the feature contains complex values that need special handling\n",
    "    if X['Sex'].apply(lambda x: isinstance(x, (dict, list))).any():\n",
    "        print(f\"Warning: Cannot apply one-hot encoding to complex objects in Sex\")\n",
    "        print(f\"Converting to string representation first\")\n",
    "        X['Sex'] = X['Sex'].astype(str)\n",
    "    try:\n",
    "        onehot = pd.get_dummies(X['Sex'], prefix='Sex')\n",
    "        X = pd.concat([X.drop('Sex', axis=1), onehot], axis=1)\n",
    "        print(f\"Created one-hot encoding for: Sex\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error one-hot encoding Sex: {e}\")\n",
    "# Apply one-hot encoding to Ticket\n",
    "if 'Ticket' in categorical_features:\n",
    "    # Check if the feature contains complex values that need special handling\n",
    "    if X['Ticket'].apply(lambda x: isinstance(x, (dict, list))).any():\n",
    "        print(f\"Warning: Cannot apply one-hot encoding to complex objects in Ticket\")\n",
    "        print(f\"Converting to string representation first\")\n",
    "        X['Ticket'] = X['Ticket'].astype(str)\n",
    "    try:\n",
    "        onehot = pd.get_dummies(X['Ticket'], prefix='Ticket')\n",
    "        X = pd.concat([X.drop('Ticket', axis=1), onehot], axis=1)\n",
    "        print(f\"Created one-hot encoding for: Ticket\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error one-hot encoding Ticket: {e}\")\n",
    "# Apply one-hot encoding to Cabin\n",
    "if 'Cabin' in categorical_features:\n",
    "    # Check if the feature contains complex values that need special handling\n",
    "    if X['Cabin'].apply(lambda x: isinstance(x, (dict, list))).any():\n",
    "        print(f\"Warning: Cannot apply one-hot encoding to complex objects in Cabin\")\n",
    "        print(f\"Converting to string representation first\")\n",
    "        X['Cabin'] = X['Cabin'].astype(str)\n",
    "    try:\n",
    "        onehot = pd.get_dummies(X['Cabin'], prefix='Cabin')\n",
    "        X = pd.concat([X.drop('Cabin', axis=1), onehot], axis=1)\n",
    "        print(f\"Created one-hot encoding for: Cabin\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error one-hot encoding Cabin: {e}\")\n",
    "# Apply one-hot encoding to Embarked\n",
    "if 'Embarked' in categorical_features:\n",
    "    # Check if the feature contains complex values that need special handling\n",
    "    if X['Embarked'].apply(lambda x: isinstance(x, (dict, list))).any():\n",
    "        print(f\"Warning: Cannot apply one-hot encoding to complex objects in Embarked\")\n",
    "        print(f\"Converting to string representation first\")\n",
    "        X['Embarked'] = X['Embarked'].astype(str)\n",
    "    try:\n",
    "        onehot = pd.get_dummies(X['Embarked'], prefix='Embarked')\n",
    "        X = pd.concat([X.drop('Embarked', axis=1), onehot], axis=1)\n",
    "        print(f\"Created one-hot encoding for: Embarked\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error one-hot encoding Embarked: {e}\")\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "    \n",
    "    # Create preprocessing pipelines for numerical and categorical features\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # Combine preprocessing pipelines\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af89d99",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ff30b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = LogisticRegression(random_state=42)\n",
    "\n",
    "    # Create a pipeline with preprocessing and model\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Fit the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b139bf",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702adb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        # Evaluate the model performance\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        # Get classification report\n",
    "        class_report = classification_report(y_test, y_pred)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(class_report)\n",
    "        \n",
    "        # Create confusion matrix\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # If binary classification, create ROC curve\n",
    "        if len(np.unique(y)) == 2:\n",
    "            try:\n",
    "                y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "                fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "                auc_score = roc_auc_score(y_test, y_prob)\n",
    "                \n",
    "                plt.figure(figsize=(8, 6))\n",
    "                plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc_score:.4f})')\n",
    "                plt.plot([0, 1], [0, 1], 'r--')\n",
    "                plt.xlabel('False Positive Rate')\n",
    "                plt.ylabel('True Positive Rate')\n",
    "                plt.title('ROC Curve')\n",
    "                plt.legend(loc='lower right')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            except:\n",
    "                print(\"Could not generate ROC curve.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c99cf1d",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaac7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Try to get feature importance if available\n",
    "    try:\n",
    "        if hasattr(pipeline['model'], 'feature_importances_'):\n",
    "            # Get feature names from the preprocessor\n",
    "            feature_names = []\n",
    "            for name, trans, cols in pipeline['preprocessor'].transformers_:\n",
    "                if name == 'cat':\n",
    "                    # Get the one-hot encoded feature names\n",
    "                    cat_features = trans.named_steps['onehot'].get_feature_names_out(cols)\n",
    "                    feature_names.extend(cat_features)\n",
    "                else:\n",
    "                    feature_names.extend(cols)\n",
    "            \n",
    "            # Get feature importances from the model\n",
    "            importances = pipeline['model'].feature_importances_\n",
    "            \n",
    "            # Create a dataframe with feature importances\n",
    "            if len(feature_names) == len(importances):\n",
    "                feature_importance_df = pd.DataFrame({\n",
    "                    'Feature': feature_names,\n",
    "                    'Importance': importances\n",
    "                }).sort_values('Importance', ascending=False)\n",
    "                \n",
    "                # Plot feature importances\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(15))\n",
    "                plt.title('Feature Importance')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        else:\n",
    "            print(\"Feature importances not available for this model.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not compute feature importances: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aba3ba4",
   "metadata": {},
   "source": [
    "## SHAP Values for Model Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ce9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Try to compute SHAP values for model explainability\n",
    "    try:\n",
    "        import shap\n",
    "        \n",
    "        # Sample a subset of the test data for SHAP analysis (for performance)\n",
    "        n_samples = min(100, X_test.shape[0])\n",
    "        X_sample = X_test.iloc[:n_samples]\n",
    "        \n",
    "        # Create a SHAP explainer\n",
    "        if hasattr(pipeline['model'], 'predict_proba'):\n",
    "            explainer = shap.Explainer(pipeline['model'], pipeline['preprocessor'].transform(X_sample))\n",
    "            shap_values = explainer(pipeline['preprocessor'].transform(X_sample))\n",
    "        else:\n",
    "            explainer = shap.Explainer(pipeline['model'], pipeline['preprocessor'].transform(X_sample))\n",
    "            shap_values = explainer(pipeline['preprocessor'].transform(X_sample))\n",
    "        \n",
    "        # Summary plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values, pipeline['preprocessor'].transform(X_sample))\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Could not compute SHAP values: {e}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b04fac5",
   "metadata": {},
   "source": [
    "## Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7708df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Save the model and preprocessor to files\n",
    "    import pickle\n",
    "    \n",
    "    # Create directory for model export\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    \n",
    "    # Save the model\n",
    "    with open('models/model.pkl', 'wb') as f:\n",
    "        pickle.dump(pipeline, f)\n",
    "    \n",
    "    print(\"Model saved successfully!\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e04112",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8458a6cf",
   "metadata": {},
   "source": [
    "\n",
    "        This notebook demonstrated how to:\n",
    "        \n",
    "        1. Load and explore the dataset\n",
    "        2. Preprocess the data for machine learning\n",
    "        3. Train a logistic_regression model for classification\n",
    "        4. Evaluate the model's performance\n",
    "        5. Understand feature importance and model explainability\n",
    "        6. Export the model for deployment\n",
    "        \n",
    "        The model can be improved by:\n",
    "        \n",
    "        * Feature engineering\n",
    "        * Hyperparameter tuning\n",
    "        * Trying different algorithms\n",
    "        * Collecting more data\n",
    "        "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
