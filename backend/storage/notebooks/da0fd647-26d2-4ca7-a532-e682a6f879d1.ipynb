{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb35b4a",
   "metadata": {},
   "source": [
    "# Machine Learning Model: ModelType.LOGISTIC_REGRESSION for TaskType.CLASSIFICATION\n",
    "    \n",
    "Generated on 2025-03-11 09:19:26\n",
    "\n",
    "This notebook demonstrates an end-to-end machine learning workflow for classification using a logistic regression model.\n",
    "\n",
    "**Dataset:** ba6aa228-bd5e-457f-9b99-bee9d2a908a5\n",
    "**Target Variable:** Survived\n",
    "**Task Type:** TaskType.CLASSIFICATION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ec9c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure visualizations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "%matplotlib inline\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa2f0b",
   "metadata": {},
   "source": [
    "## Data Loading and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c7b88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# In a production environment, this would load from a file path or database\n",
    "# For this notebook, we're using the data that was uploaded\n",
    "df = pd.read_csv('data.csv')  # Placeholder - this will be replaced with actual data\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nSummary statistics:\")\n",
    "display(df.describe(include='all').T)\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values per column:\")\n",
    "display(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd421351",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d3e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "target_counts = df['Survived'].value_counts()\n",
    "sns.barplot(x=target_counts.index, y=target_counts.values)\n",
    "plt.title('Distribution of Target Classes')\n",
    "plt.xlabel('Survived')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation analysis for numerical features\n",
    "numeric_df = df.select_dtypes(include=['number'])\n",
    "if numeric_df.shape[1] > 1:  # Only if we have numeric features\n",
    "    corr = numeric_df.corr()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "    sns.heatmap(corr, mask=mask, cmap='coolwarm', annot=True, fmt='.2f', square=True)\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Feature distribution by class\n",
    "numerical_features = df.select_dtypes(include=['number']).columns.tolist()\n",
    "numerical_features = [f for f in numerical_features if f != 'Survived'][:3]  # Top 3 numerical features\n",
    "\n",
    "if numerical_features:\n",
    "    fig, axes = plt.subplots(len(numerical_features), 1, figsize=(12, 4*len(numerical_features)))\n",
    "    if len(numerical_features) == 1:\n",
    "        axes = [axes]  # Make axes iterable if only one feature\n",
    "        \n",
    "    for i, feature in enumerate(numerical_features):\n",
    "        sns.boxplot(x='Survived', y=feature, data=df, ax=axes[i])\n",
    "        axes[i].set_title(f'{feature} by Survived')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c1083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a0885a",
   "metadata": {},
   "source": [
    "## Data Preprocessing and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751e21e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and target\n",
    "X = df.drop(columns=['Survived'])\n",
    "y = df['Survived']\n",
    "\n",
    "# Identify numerical and categorical features\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical features: {numerical_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Check for complex object features that may cause issues\n",
    "for col in categorical_features:\n",
    "    sample = df[col].iloc[0]\n",
    "    if isinstance(sample, dict) or isinstance(sample, list):\n",
    "        print(f\"Warning: Column {col} contains complex objects which may not work with standard transformations.\")\n",
    "        print(f\"Sample value: {sample}\")\n",
    "        print(f\"Consider extracting specific fields from this object or excluding it from your model.\")\n",
    "\n",
    "# Apply one-hot encoding to Name\n",
    "if 'Name' in categorical_features:\n",
    "    # Check if the feature contains complex values that need special handling\n",
    "    if X['Name'].apply(lambda x: isinstance(x, (dict, list))).any():\n",
    "        print(f\"Warning: Cannot apply one-hot encoding to complex objects in Name\")\n",
    "        print(f\"Converting to string representation first\")\n",
    "        X['Name'] = X['Name'].astype(str)\n",
    "    try:\n",
    "        onehot = pd.get_dummies(X['Name'], prefix='Name')\n",
    "        X = pd.concat([X.drop('Name', axis=1), onehot], axis=1)\n",
    "        print(f\"Created one-hot encoding for: Name\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error one-hot encoding Name: {e}\")\n",
    "# Apply one-hot encoding to Sex\n",
    "if 'Sex' in categorical_features:\n",
    "    # Check if the feature contains complex values that need special handling\n",
    "    if X['Sex'].apply(lambda x: isinstance(x, (dict, list))).any():\n",
    "        print(f\"Warning: Cannot apply one-hot encoding to complex objects in Sex\")\n",
    "        print(f\"Converting to string representation first\")\n",
    "        X['Sex'] = X['Sex'].astype(str)\n",
    "    try:\n",
    "        onehot = pd.get_dummies(X['Sex'], prefix='Sex')\n",
    "        X = pd.concat([X.drop('Sex', axis=1), onehot], axis=1)\n",
    "        print(f\"Created one-hot encoding for: Sex\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error one-hot encoding Sex: {e}\")\n",
    "# Apply one-hot encoding to Ticket\n",
    "if 'Ticket' in categorical_features:\n",
    "    # Check if the feature contains complex values that need special handling\n",
    "    if X['Ticket'].apply(lambda x: isinstance(x, (dict, list))).any():\n",
    "        print(f\"Warning: Cannot apply one-hot encoding to complex objects in Ticket\")\n",
    "        print(f\"Converting to string representation first\")\n",
    "        X['Ticket'] = X['Ticket'].astype(str)\n",
    "    try:\n",
    "        onehot = pd.get_dummies(X['Ticket'], prefix='Ticket')\n",
    "        X = pd.concat([X.drop('Ticket', axis=1), onehot], axis=1)\n",
    "        print(f\"Created one-hot encoding for: Ticket\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error one-hot encoding Ticket: {e}\")\n",
    "# Apply one-hot encoding to Cabin\n",
    "if 'Cabin' in categorical_features:\n",
    "    # Check if the feature contains complex values that need special handling\n",
    "    if X['Cabin'].apply(lambda x: isinstance(x, (dict, list))).any():\n",
    "        print(f\"Warning: Cannot apply one-hot encoding to complex objects in Cabin\")\n",
    "        print(f\"Converting to string representation first\")\n",
    "        X['Cabin'] = X['Cabin'].astype(str)\n",
    "    try:\n",
    "        onehot = pd.get_dummies(X['Cabin'], prefix='Cabin')\n",
    "        X = pd.concat([X.drop('Cabin', axis=1), onehot], axis=1)\n",
    "        print(f\"Created one-hot encoding for: Cabin\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error one-hot encoding Cabin: {e}\")\n",
    "# Apply one-hot encoding to Embarked\n",
    "if 'Embarked' in categorical_features:\n",
    "    # Check if the feature contains complex values that need special handling\n",
    "    if X['Embarked'].apply(lambda x: isinstance(x, (dict, list))).any():\n",
    "        print(f\"Warning: Cannot apply one-hot encoding to complex objects in Embarked\")\n",
    "        print(f\"Converting to string representation first\")\n",
    "        X['Embarked'] = X['Embarked'].astype(str)\n",
    "    try:\n",
    "        onehot = pd.get_dummies(X['Embarked'], prefix='Embarked')\n",
    "        X = pd.concat([X.drop('Embarked', axis=1), onehot], axis=1)\n",
    "        print(f\"Created one-hot encoding for: Embarked\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error one-hot encoding Embarked: {e}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Create preprocessing pipelines for numerical and categorical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14b658",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5f2dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Create a pipeline with preprocessing and model\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"Model training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61db9dcb",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9514990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate classification metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=np.unique(y_test),\n",
    "            yticklabels=np.unique(y_test))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC curve and AUC (for binary classification)\n",
    "if len(np.unique(y_test)) == 2:\n",
    "    try:\n",
    "        y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        auc = roc_auc_score(y_test, y_prob)\n",
    "        \n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f'AUC = {auc:.3f}')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nAUC: {auc:.4f}\")\n",
    "    except:\n",
    "        print(\"Could not calculate ROC curve and AUC.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129972dc",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f763a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to get feature importance if available\n",
    "try:\n",
    "    if hasattr(pipeline['model'], 'feature_importances_'):\n",
    "        # Get feature names from the preprocessor\n",
    "        feature_names = []\n",
    "        for name, trans, cols in pipeline['preprocessor'].transformers_:\n",
    "            if name == 'cat' and cols:\n",
    "                # Get the one-hot encoded feature names\n",
    "                cat_features = trans.named_steps['onehot'].get_feature_names_out(cols)\n",
    "                feature_names.extend(cat_features)\n",
    "            else:\n",
    "                feature_names.extend(cols)\n",
    "        \n",
    "        # Get feature importances from the model\n",
    "        importances = pipeline['model'].feature_importances_\n",
    "        \n",
    "        # Create a dataframe with feature importances\n",
    "        if len(feature_names) == len(importances):\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': importances\n",
    "            }).sort_values(by='Importance', ascending=False)\n",
    "            \n",
    "            # Plot feature importances\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(15))\n",
    "            plt.title('Feature Importance')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(feature_importance_df.head(15))\n",
    "    else:\n",
    "        print(\"Feature importances not available for this model.\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not compute feature importances: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb341096",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f780c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the model\n",
    "print(\"Model Training Summary:\")\n",
    "print(f\"Dataset: {len(X_train) + len(X_test)} samples ({len(X_train)} train, {len(X_test)} test)\")\n",
    "print(f\"Task Type: TaskType.CLASSIFICATION\")\n",
    "print(f\"Model Type: ModelType.LOGISTIC_REGRESSION\")\n",
    "print(f\"Features Used: {len(X.columns)}\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"1. Try different feature transformations to improve performance\")\n",
    "print(\"2. Experiment with hyperparameter tuning to find optimal model settings\")\n",
    "print(\"3. Consider feature selection to focus on the most important variables\")\n",
    "print(\"4. For deployment, save the model using joblib or pickle\")\n",
    "print(\"5. Monitor model performance over time and retrain as needed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db525000",
   "metadata": {},
   "source": [
    "## SHAP Values for Model Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0572caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to compute SHAP values for model explainability\n",
    "try:\n",
    "    import shap\n",
    "    \n",
    "    # Sample a subset of the test data for SHAP analysis (for performance)\n",
    "    n_samples = min(100, X_test.shape[0])\n",
    "    X_sample = X_test.iloc[:n_samples]\n",
    "    \n",
    "    # Create a SHAP explainer\n",
    "    try:\n",
    "        # For sklearn models\n",
    "        explainer = shap.Explainer(pipeline['model'], pipeline['preprocessor'].transform(X_sample))\n",
    "        shap_values = explainer(pipeline['preprocessor'].transform(X_sample))\n",
    "        \n",
    "        # Summary plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(shap_values, pipeline['preprocessor'].transform(X_sample))\n",
    "        plt.tight_layout()\n",
    "    except Exception as shap_error:\n",
    "        print(f\"First SHAP approach failed: {shap_error}\")\n",
    "        \n",
    "        # Try alternative approach for tree-based models\n",
    "        if hasattr(pipeline['model'], 'feature_importances_'):\n",
    "            explainer = shap.TreeExplainer(pipeline['model'])\n",
    "            # Transform the data first\n",
    "            X_transformed = pipeline['preprocessor'].transform(X_sample)\n",
    "            shap_values = explainer.shap_values(X_transformed)\n",
    "            \n",
    "            plt.figure(figsize=(10, 8))\n",
    "            if isinstance(shap_values, list):  # For multi-class classification\n",
    "                shap.summary_plot(shap_values[0], X_transformed)\n",
    "            else:  # For regression or binary classification\n",
    "                shap.summary_plot(shap_values, X_transformed)\n",
    "            plt.tight_layout()\n",
    "        else:\n",
    "            print(\"Model type not supported for detailed SHAP analysis\")\n",
    "            \n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Could not compute SHAP values: {e}\")\n",
    "    print(\"Note: SHAP analysis requires additional setup in some environments.\")\n",
    "    print(\"If you want to use SHAP, try installing it separately with: pip install shap\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6c553b",
   "metadata": {},
   "source": [
    "## Model Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e36fac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file\n",
    "import joblib\n",
    "\n",
    "try:\n",
    "    # Save the pipeline (includes preprocessor and model)\n",
    "    joblib.dump(pipeline, 'trained_model_pipeline.joblib')\n",
    "    print(\"Model pipeline saved successfully!\")\n",
    "    \n",
    "    # How to load the model\n",
    "    print(\"\\nTo load and use this model in another script:\")\n",
    "    print(\"import joblib\")\n",
    "    print(\"loaded_pipeline = joblib.load('trained_model_pipeline.joblib')\")\n",
    "    print(\"predictions = loaded_pipeline.predict(new_data)\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving model: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946e6344",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4b46c2",
   "metadata": {},
   "source": [
    "This notebook demonstrated how to:\n",
    "\n",
    "1. Load and explore the dataset\n",
    "2. Preprocess the data for machine learning\n",
    "3. Train a ModelType.LOGISTIC_REGRESSION model for TaskType.CLASSIFICATION\n",
    "4. Evaluate the model's performance \n",
    "5. Understand feature importance and model explainability\n",
    "6. Export the model for deployment\n",
    "\n",
    "The model can be improved by:\n",
    "\n",
    "* Feature engineering and selection\n",
    "* Hyperparameter tuning\n",
    "* Trying different algorithms\n",
    "* Collecting more data\n",
    "* Addressing class imbalance (if present)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
